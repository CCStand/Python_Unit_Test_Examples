{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is unit testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit testing is the practice of creating 'tests' that allow you to automatically check that individual parts of your code are working as expected.\n",
    "\n",
    "Unit testing should be done at the level of individual functions or methods so that if your code fails a test, you can quickly identify where the error is coming from. It can still be useful to create tests that check that your code works when you try and run everything together, but this should be done alongside unit testing, and not instead of unit testing! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we do unit testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit testing allows you to:\n",
    "* Identify if your code is broken\n",
    "* Identify where and in what way your code is broken\n",
    "* Save you time by doing this automatically instead of requiring you to manually run lines of code (this is particularly useful in collaborative projects where you might want to test code that somebody else has written before you merge a change into your main branch)\n",
    "\n",
    "Taking unit tests can take time (especially when you've only just started using them), but the more unit tests you write for your project, the faster you'll be able to fix problems as the project gets bigger, and the more confident you (and others) will be that your code actually does what you think it does.\n",
    "\n",
    "If you've never done unit testing before, you don't need to paralyse yourself by making a unit test for everything! It's great to have lots of unit tests, but even only having some unit tests is better than having no unit tests! Particularly for Data Science projects, you might not necessarily know what output to expect from a piece of code you write, making it difficult to write a unit test, just try your best to write tests wherever you can!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we do unit testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of ways to write to write unit tests. The one I will focus on here is `pytest`, but other tools are available and you should feel free to use whatever tools suit you best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to import a few packages to use `pytest`, namely `pytest` itself!\n",
    "\n",
    "So that I can show the output of tests within this notebook, I'm also using a package called [`ipytest`](https://github.com/chmp/ipytest/blob/main/Example.ipynb), but most of the time you'll want to simply run pytest from the terminal and won't require this package.\n",
    "\n",
    "`hypothesis` is a package that includes some more advanced testing features. We'll come back to look at `hypothesis` later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from hypothesis import given, strategies as st\n",
    "\n",
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple function that adds one number to another number..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def adds_x(num, x):\n",
    "    return(num + x)\n",
    "\n",
    "print(adds_x(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this function with a unit test, we need to write a unit test function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adds_x():\n",
    "    result = adds_x(3, 1)\n",
    "    expect = 4\n",
    "    assert result == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three key parts to this test:\n",
    "* **The name of the test function** - It is important that the test name begins with `test_`! This allows pytest to identify the tests associated with your package and automatically run them all for you. The function name should also make it clear what you are testing.\n",
    "* **The code that tests your function** - Obviously, if we want to test a function, we'll have to use that function within the test and compare the result we get to the result we expect and make sure they're the same.\n",
    "* **The `assert` statement** - This line determines whether your test passess or fails. If the statement is true, then the test passes. If the statement is false, then the test fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, you'll be running `pytest` by using the `pytest` command from the terminal within your repo (we'll go through how this works later). However, in this example, I will run the test from within the notebook so you can see the test output in the same place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[33m                                                                                            [100%]\u001b[0m\n",
      "\u001b[33m======================================== warnings summary =========================================\u001b[0m\n",
      "..\\..\\..\\..\\..\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126\n",
      "  c:\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: hypothesis\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m\u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_adds_x():\n",
    "    result = adds_x(3, 1)\n",
    "    expect = 4\n",
    "    assert result == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our test passed! This is because the assert statement was true (i.e. the result produced by our function was the result that we were expecting)!\n",
    "\n",
    "Running the test will also display warning by defaults, even if your test passes.\n",
    "\n",
    "But what if our test fails?..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
      "============================================ FAILURES =============================================\n",
      "\u001b[31m\u001b[1m________________________________________ test_adds_x_fail _________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_adds_x_fail\u001b[39;49;00m():\n",
      "        result = adds_x(\u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m) + \u001b[94m1\u001b[39;49;00m\n",
      "        expect = \u001b[94m4\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m result == expect\n",
      "\u001b[1m\u001b[31mE       assert 5 == 4\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m<ipython-input-4-8c5fc7a60feb>\u001b[0m:4: AssertionError\n",
      "\u001b[33m======================================== warnings summary =========================================\u001b[0m\n",
      "..\\..\\..\\..\\..\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126\n",
      "  c:\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: hypothesis\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "===================================== short test summary info =====================================\n",
      "FAILED tmpjo7wo22c.py::test_adds_x_fail - assert 5 == 4\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m1 warning\u001b[0m\u001b[31m in 0.31s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_adds_x_fail():\n",
    "    result = adds_x(3, 1) + 1\n",
    "    expect = 4\n",
    "    assert result == expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our test failed (because the assert statement was false). This tells us which test failed, but if we want, we can also add a string after the assert statement that will tell you what went wrong when the test fails (great if you've got a complicated test case and you might forget what it tests for)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
      "============================================ FAILURES =============================================\n",
      "\u001b[31m\u001b[1m________________________________________ test_adds_x_fail _________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_adds_x_fail\u001b[39;49;00m():\n",
      "        result = adds_x(\u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m) + \u001b[94m1\u001b[39;49;00m\n",
      "        expect = \u001b[94m4\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m result == expect, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33madds_x should return 4 when given 3 and 1, but instead returned \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mresult\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: adds_x should return 4 when given 3 and 1, but instead returned 5\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 5 == 4\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m<ipython-input-5-f44bbbd21ef7>\u001b[0m:4: AssertionError\n",
      "\u001b[33m======================================== warnings summary =========================================\u001b[0m\n",
      "..\\..\\..\\..\\..\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126\n",
      "  c:\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: hypothesis\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "===================================== short test summary info =====================================\n",
      "FAILED tmp3vqyvwnv.py::test_adds_x_fail - AssertionError: adds_x should return 4 when given 3 and...\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m1 warning\u001b[0m\u001b[31m in 0.03s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_adds_x_fail():\n",
    "    result = adds_x(3, 1) + 1\n",
    "    expect = 4\n",
    "    assert result == expect, f\"adds_x should return 4 when given 3 and 1, but instead returned {result}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating tests (a bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, the test inputs and outputs were hard-coded in. But we can use a feature of `pytest` to define a series of inputs. This allows you to automatically run a series of tests without having to write a new test function to test a different input.\n",
    "\n",
    "We do this by using a `parametrize` decorator (the line of code that starts with an '@') in front of the test function that tells the test to use a defined series of inputs. You have to pass this decorator two things:\n",
    "* A string that contains the names of the parameters to automate the input for\n",
    "* A list of tuples, where each tuple comtains the values you want to pass in one iteration of the test (the values need to be given in the same order that you named the parameters in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                          [100%]\u001b[0m\n",
      "============================================ FAILURES =============================================\n",
      "\u001b[31m\u001b[1m________________________________________ test_adds_x[0--1] ________________________________________\u001b[0m\n",
      "\n",
      "num = 0, x = -1\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum, x\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [(\u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), (\u001b[94m0\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m), (-\u001b[94m5\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m)])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_adds_x\u001b[39;49;00m(num, x):\n",
      "        result = adds_x(num, x)\n",
      "        expect = num+\u001b[94m1\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m result == expect, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33madds_x should return \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mexpect\u001b[33m}\u001b[39;49;00m\u001b[33m when given \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mnum\u001b[33m}\u001b[39;49;00m\u001b[33m and \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mx\u001b[33m}\u001b[39;49;00m\u001b[33m, but instead returned \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mresult\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: adds_x should return 1 when given 0 and -1, but instead returned -1\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert -1 == 1\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m<ipython-input-6-d7958550b5a1>\u001b[0m:5: AssertionError\n",
      "\u001b[31m\u001b[1m________________________________________ test_adds_x[-5-2] ________________________________________\u001b[0m\n",
      "\n",
      "num = -5, x = 2\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum, x\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [(\u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), (\u001b[94m0\u001b[39;49;00m, -\u001b[94m1\u001b[39;49;00m), (-\u001b[94m5\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m)])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_adds_x\u001b[39;49;00m(num, x):\n",
      "        result = adds_x(num, x)\n",
      "        expect = num+\u001b[94m1\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m result == expect, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33madds_x should return \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mexpect\u001b[33m}\u001b[39;49;00m\u001b[33m when given \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mnum\u001b[33m}\u001b[39;49;00m\u001b[33m and \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mx\u001b[33m}\u001b[39;49;00m\u001b[33m, but instead returned \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mresult\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: adds_x should return -4 when given -5 and 2, but instead returned -3\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert -3 == -4\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m<ipython-input-6-d7958550b5a1>\u001b[0m:5: AssertionError\n",
      "\u001b[33m======================================== warnings summary =========================================\u001b[0m\n",
      "..\\..\\..\\..\\..\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126\n",
      "  c:\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: hypothesis\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "===================================== short test summary info =====================================\n",
      "FAILED tmp1vyn48az.py::test_adds_x[0--1] - AssertionError: adds_x should return 1 when given 0 an...\n",
      "FAILED tmp1vyn48az.py::test_adds_x[-5-2] - AssertionError: adds_x should return -4 when given -5 ...\n",
      "\u001b[31m\u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m1 passed\u001b[0m, \u001b[33m1 warning\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "@pytest.mark.parametrize(\"num, x\", [(3, 1), (0, -1), (-5, 2)])\n",
    "def test_adds_x(num, x):\n",
    "    result = adds_x(num, x)\n",
    "    expect = num+1\n",
    "    assert result == expect, f\"adds_x should return {expect} when given {num} and {x}, but instead returned {result}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that instead of just running one test, you actually just ran three! If you look at the first line, you can see a green '.' and two red 'F's, this means that the 1st test passed while the 2nd and 3rd tests failed.\n",
    "\n",
    "If it makes sense for your test, you can use a mixture of hard-coding and parametrizing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                                                         [100%]\u001b[0m\n",
      "\u001b[33m======================================== warnings summary =========================================\u001b[0m\n",
      "..\\..\\..\\..\\..\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126\n",
      "  c:\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: hypothesis\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m\u001b[32m4 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 0.05s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "@pytest.mark.parametrize(\"num\", [(-1), (0), (1), (100000000)])\n",
    "def test_adds_one(num):\n",
    "    x = 1\n",
    "    result = adds_x(num, x)\n",
    "    expect = num + x\n",
    "    assert result == expect, f\"adds_x should return {expect} when given {num} and {x}, but instead returned {result}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can I make unit testing less of a slog?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even using parametrization can take a lot of time! You need to think of and type out as many edge cases as you can think of, so you're likely to only be feasibly able to test a small subset of possible cases.\n",
    "\n",
    "We can use the `hypothesis` package in Python to simplify this for us! `hypothesis` uses property-based testing, which just means it can perform tests on a particular type of input rather than a set of inputs you explicitly define.\n",
    "\n",
    "We do this by using a `given` decorator in front of the test function that tells the test what sort of inputs to test. Unlike `pytest`'s `parametrize` decorator, you don't need to name the parameters you're passing, the arguments are simply passed in order.\n",
    "\n",
    "Let's look at a test function that uses the `given` decorator..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[33m                                                                                            [100%]\u001b[0m\n",
      "\u001b[33m======================================== warnings summary =========================================\u001b[0m\n",
      "..\\..\\..\\..\\..\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126\n",
      "  c:\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: hypothesis\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m\u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 0.15s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "@given(st.integers())\n",
    "def test_adds_one(num):\n",
    "    x = 1\n",
    "    result = adds_x(num, x)\n",
    "    expect = num + x\n",
    "    assert result == expect, f\"adds_x should return {expect} when given {num} and {x}, but instead returned {result}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to have run a single test (only one green '.'), but if we take a look at what's actually happening..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 0...\n",
      "Trying 0...\n",
      "Trying 21108...\n",
      "Trying 0...\n",
      "Trying -13061...\n",
      "Trying 0...\n",
      "Trying 344233119867605228...\n",
      "Trying 0...\n",
      "Trying 0...\n",
      "Trying 0...\n",
      "Trying -16026...\n",
      "Trying -62...\n",
      "Trying 26095...\n",
      "Trying -101...\n",
      "Trying -13299...\n",
      "Trying -51...\n",
      "Trying -97356405197373059265829707354439238195...\n",
      "Trying -9917...\n",
      "Trying 384...\n",
      "Trying -4579...\n",
      "Trying -111...\n",
      "Trying 29...\n",
      "Trying 2024...\n",
      "Trying 33...\n",
      "Trying -2180053...\n",
      "Trying -1054235964690920472...\n",
      "Trying -27444...\n",
      "Trying 8418...\n",
      "Trying -32...\n",
      "Trying -2829...\n",
      "Trying -25348...\n",
      "Trying 24945...\n",
      "Trying 97...\n",
      "Trying -8840857843637675326594082683253669381...\n",
      "Trying 13...\n",
      "Trying -2731...\n",
      "Trying -10...\n",
      "Trying 99847871871469761951544532910454698335...\n",
      "Trying -359670906...\n",
      "Trying -9798...\n",
      "Trying 38...\n",
      "Trying -72...\n",
      "Trying 1880091742...\n",
      "Trying 83...\n",
      "Trying -5737...\n",
      "Trying -29508...\n",
      "Trying 115...\n",
      "Trying 1710447166...\n",
      "Trying 74...\n",
      "Trying -817859607...\n",
      "Trying -168290035611739574479419243320659295672...\n",
      "Trying 4214...\n",
      "Trying 19126...\n",
      "Trying -31494...\n",
      "Trying 103...\n",
      "Trying 24636...\n",
      "Trying -26064...\n",
      "Trying -43...\n",
      "Trying 6897302297052061666...\n",
      "Trying -19486...\n",
      "Trying 76...\n",
      "Trying 14765...\n",
      "Trying -57...\n",
      "Trying -16183513...\n",
      "Trying -65717394398741409395003533422017452382...\n",
      "Trying -16700...\n",
      "Trying -12561...\n",
      "Trying 5...\n",
      "Trying -6158212977056295724...\n",
      "Trying 1926783737...\n",
      "Trying -1779...\n",
      "Trying 17282...\n",
      "Trying -67...\n",
      "Trying -73...\n",
      "Trying -1565...\n",
      "Trying -17693...\n",
      "Trying -2704...\n",
      "Trying -31064...\n",
      "Trying 12147...\n",
      "Trying -2100726577...\n",
      "Trying 4535...\n",
      "Trying -23587...\n",
      "Trying -19632...\n",
      "Trying -8677...\n",
      "Trying 7407...\n",
      "Trying -164451175062504675925073448932125385549...\n",
      "Trying -665...\n",
      "Trying -2...\n",
      "Trying 5611...\n",
      "Trying -12162966530961702...\n",
      "Trying 110...\n",
      "Trying -20541...\n",
      "Trying 80...\n",
      "Trying -5125755319224941191...\n",
      "Trying -9056829231250901352...\n",
      "Trying 472...\n",
      "Trying -1...\n",
      "Trying -1697862267...\n",
      "Trying -112...\n",
      "Trying -19547...\n",
      "\u001b[32m.\u001b[0m\n",
      "\u001b[33m======================================== warnings summary =========================================\u001b[0m\n",
      "..\\..\\..\\..\\..\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126\n",
      "  c:\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: hypothesis\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m\u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 0.17s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "\n",
    "@given(st.integers())\n",
    "def test_adds_one(num):\n",
    "    print(f\"Trying {num}...\")\n",
    "    x = 1\n",
    "    result = adds_x(num, x)\n",
    "    expect = num + x\n",
    "    assert result == expect, f\"adds_x should return {expect} when given {num} and {x}, but instead returned {result}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can include print statements in your tests. By default, this won't display in your `pytest` output, but if you run `pytest` with the `-s` flag, any print statements will be displayed as your tests run!\n",
    "\n",
    "This 'one' test is actually running 100 different tests of auto-generated numbers that cover a large range of cases (0, negative numbers, large numbers, large negative numbers...), but condesing the result into one output to prevent clutering your test results.\n",
    "\n",
    "If the test fails on any input, it will show you the output that the test failed on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
      "============================================ FAILURES =============================================\n",
      "\u001b[31m\u001b[1m___________________________________ test_adds_one_fail_on_zero ____________________________________\u001b[0m\n",
      "\n",
      "    \u001b[37m@given\u001b[39;49;00m(st.integers())\n",
      ">   \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_adds_one_fail_on_zero\u001b[39;49;00m(num):\n",
      "\n",
      "\u001b[1m\u001b[31m<ipython-input-10-f087c17378cc>\u001b[0m:2: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "num = 0\n",
      "\n",
      "    \u001b[37m@given\u001b[39;49;00m(st.integers())\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_adds_one_fail_on_zero\u001b[39;49;00m(num):\n",
      "        x = \u001b[94m1\u001b[39;49;00m\n",
      "        result = adds_x(num, x)\n",
      "        \u001b[94mif\u001b[39;49;00m num == \u001b[94m0\u001b[39;49;00m:\n",
      "            result = \u001b[94m0\u001b[39;49;00m\n",
      "        expect = num + x\n",
      ">       \u001b[94massert\u001b[39;49;00m result == expect, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33madds_x should return \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mexpect\u001b[33m}\u001b[39;49;00m\u001b[33m when given \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mnum\u001b[33m}\u001b[39;49;00m\u001b[33m and \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mx\u001b[33m}\u001b[39;49;00m\u001b[33m, but instead returned \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mresult\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: adds_x should return 1 when given 0 and 1, but instead returned 0\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 0 == 1\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m<ipython-input-10-f087c17378cc>\u001b[0m:8: AssertionError\n",
      "------------------------------------------- Hypothesis --------------------------------------------\n",
      "Falsifying example: test_adds_one_fail_on_zero(\n",
      "    num=0,\n",
      ")\n",
      "\u001b[33m======================================== warnings summary =========================================\u001b[0m\n",
      "..\\..\\..\\..\\..\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126\n",
      "  c:\\python36\\lib\\site-packages\\_pytest\\config\\__init__.py:1126: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: hypothesis\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "tmp8y883327.py::test_adds_one_fail_on_zero\n",
      "tmp8y883327.py::test_adds_one_fail_on_zero\n",
      "  c:\\python36\\lib\\site-packages\\hypothesis\\internal\\escalation.py:131: PytestDeprecationWarning: A private pytest class or function was used.\n",
      "    return str(item.repr_failure(ExceptionInfo((type(err), err, tb)))) + \"\\n\"\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "===================================== short test summary info =====================================\n",
      "FAILED tmp8y883327.py::test_adds_one_fail_on_zero - AssertionError: adds_x should return 1 when g...\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m3 warnings\u001b[0m\u001b[31m in 0.08s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "@given(st.integers())\n",
    "def test_adds_one_fail_on_zero(num):\n",
    "    x = 1\n",
    "    result = adds_x(num, x)\n",
    "    if num == 0:\n",
    "        result = 0\n",
    "    expect = num + x\n",
    "    assert result == expect, f\"adds_x should return {expect} when given {num} and {x}, but instead returned {result}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hypothesis` is clever, and if your test fails on a particular input value, it will make sure to include the same value in future test runs. It also has all sorts of cool features to cover a whole range of tests, so I'd recommend reading through [the docs](https://hypothesis.readthedocs.io/en/latest/)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
